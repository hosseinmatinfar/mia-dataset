from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores.chroma import Chroma
import os

data_path = "data"
db_path = "vector_db"

os.makedirs(data_path, exist_ok=True)

docs = []

print("ğŸ“‚ Loading documents...")
for f in os.listdir(data_path):
    path = os.path.join(data_path, f)
    try:
        if f.endswith(".pdf"):
            loaded = PyPDFLoader(path).load()
        elif f.endswith(".txt"):
            loaded = TextLoader(path).load()
        elif f.endswith(".docx"):
            loaded = Docx2txtLoader(path).load()
        else:
            print(f"âš ï¸ Skipping unsupported file: {f}")
            continue

        if not loaded:
            print(f"âš ï¸ No text found in {f}")
        else:
            docs.extend(loaded)
    except Exception as e:
        print(f"âŒ Error loading {f}: {e}")

print(f"ğŸ“„ Loaded {len(docs)} documents")

# ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ†â€ŒÙ‡Ø§
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)
chunks = [c for c in chunks if c.page_content.strip()]  # Ø­Ø°Ù ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ

print(f"ğŸ§© Split into {len(chunks)} chunks after cleaning")

if not chunks:
    raise ValueError("âŒ Ù‡ÛŒÚ† Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ embedding Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯. ÙØ§ÛŒÙ„â€ŒÙ‡Ø§Øª Ø±Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†.")

# Ø³Ø§Ø®Øª embedding
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø¨ÛŒØ³
db = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory=db_path
)

db.persist()
print("âœ… Ø¯ÛŒØªØ§Ø³Øª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯ Ùˆ Ø¯Ø±", db_path, "Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
