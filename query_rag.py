#!/usr/bin/env python3
"""
RAG Query System for Mia (Medical Intelligence Assistant)
Uses OpenAI API to answer questions based on pharmaceutical/medical documents
"""

import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores.chroma import Chroma
from openai import OpenAI
import sys

# Configuration
DB_PATH = "vector_db"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL = "gpt-4o-mini"  # ÛŒØ§ "gpt-4" Ø§Ú¯Ø± Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±ÛŒ

# Mia's Identity - Ø§ÛŒÙ† Ù…ØªÙ† Ø§Ø² ÙØ§ÛŒÙ„ mia_prompt Ø®ÙˆÙ†Ø¯Ù‡ Ù…ÛŒØ´Ù‡
MIA_IDENTITY = """Mia (Medical Intelligence Assistant) â€” Version 6.3 b

Description:
Mia is a multilingual, empathetic, and safety-focused AI agent developed for pharmaceutical, pharmacological, and medical education and clinical support. She combines natural conversational ability with structured, JSON-based reasoning.

Core Capabilities:
- Conversational understanding of pharmaceutical technology, pharmacology, and clinical reasoning.
- Safe, empathetic communication that avoids diagnosis or direct prescribing.

Safety & Ethics:
- Never diagnose or prescribe.
- Always add: "Final decisions must be made by a doctor or pharmacist."
- If confidence is low, ask clarifying questions instead of guessing.
"""

def load_vector_db():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ÙˆÚ©ØªÙˆØ±"""
    print("ðŸ“‚ Loading vector database...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings
    )
    print("âœ… Vector database loaded successfully")
    return db

def retrieve_context(db, query, k=5):
    """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø±ØªØ¨Ø·â€ŒØªØ±ÛŒÙ† Ø§Ø³Ù†Ø§Ø¯"""
    print(f"ðŸ” Searching for relevant documents (top {k})...")
    docs = db.similarity_search(query, k=k)

    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡
    print(f"ðŸ“š Found {len(docs)} relevant documents:")
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'Unknown')
        print(f"  {i}. {os.path.basename(source)}")

    # ØªØ±Ú©ÛŒØ¨ Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ø³Ù†Ø§Ø¯
    context = "\n\n---\n\n".join([doc.page_content for doc in docs])
    return context, docs

def query_openai(question, context, conversation_history=None):
    """Ø§Ø±Ø³Ø§Ù„ Ø³ÙˆØ§Ù„ Ø¨Ù‡ OpenAI Ø¨Ø§ context Ø§Ø² RAG"""
    if not OPENAI_API_KEY:
        raise ValueError("âŒ OPENAI_API_KEY environment variable not set!")

    client = OpenAI(api_key=OPENAI_API_KEY)

    # Ø³Ø§Ø®Øª system prompt Ø¨Ø§ Ù‡ÙˆÛŒØª Mia
    system_prompt = f"""{MIA_IDENTITY}

You are answering questions based on pharmaceutical and medical educational materials.

Instructions:
- Use the provided context to answer questions accurately
- If the answer is not in the context, say so clearly
- Always maintain Mia's empathetic and safety-focused tone
- Include the disclaimer about final decisions being made by doctors/pharmacists
- Respond in the same language as the question (English or Persian/Farsi)
"""

    # Ø³Ø§Ø®Øª user message Ø¨Ø§ context
    user_message = f"""Context from documents:
{context}

---

Question: {question}

Please answer based on the context provided above."""

    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ messages
    messages = [{"role": "system", "content": system_prompt}]

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯
    if conversation_history:
        messages.extend(conversation_history)

    messages.append({"role": "user", "content": user_message})

    print("ðŸ¤– Querying OpenAI...")
    response = client.chat.completions.create(
        model=MODEL,
        messages=messages,
        temperature=0.7,
        max_tokens=1500
    )

    answer = response.choices[0].message.content
    return answer

def interactive_mode():
    """Ø­Ø§Ù„Øª ØªØ¹Ø§Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®"""
    print("\n" + "="*60)
    print("ðŸ¥ Mia RAG System - Interactive Mode")
    print("="*60)
    print("Type 'exit' or 'quit' to end the session")
    print("Type 'clear' to clear conversation history")
    print("="*60 + "\n")

    db = load_vector_db()
    conversation_history = []

    while True:
        try:
            question = input("\nðŸ’¬ Your question: ").strip()

            if not question:
                continue

            if question.lower() in ['exit', 'quit', 'bye']:
                print("\nðŸ‘‹ Goodbye! Stay safe and healthy.")
                break

            if question.lower() == 'clear':
                conversation_history = []
                print("ðŸ—‘ï¸  Conversation history cleared")
                continue

            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            context, docs = retrieve_context(db, question, k=5)

            # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² OpenAI
            answer = query_openai(question, context, conversation_history)

            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡
            conversation_history.append({"role": "user", "content": question})
            conversation_history.append({"role": "assistant", "content": answer})

            # Ù†Ù…Ø§ÛŒØ´ Ù¾Ø§Ø³Ø®
            print("\n" + "-"*60)
            print("ðŸ¥ Mia's Response:")
            print("-"*60)
            print(answer)
            print("-"*60)

        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Session interrupted. Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ Error: {e}")

def single_query_mode(question):
    """Ø­Ø§Ù„Øª ØªÚ© Ø³ÙˆØ§Ù„"""
    db = load_vector_db()
    context, docs = retrieve_context(db, question, k=5)
    answer = query_openai(question, context)

    print("\n" + "="*60)
    print("ðŸ¥ Mia's Response:")
    print("="*60)
    print(answer)
    print("="*60 + "\n")

    print("ðŸ“š Sources used:")
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source', 'Unknown')
        print(f"  {i}. {os.path.basename(source)}")

if __name__ == "__main__":
    # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ API key
    if not OPENAI_API_KEY:
        print("âŒ Error: OPENAI_API_KEY not found in environment variables")
        print("Please set it with: export OPENAI_API_KEY='your-api-key-here'")
        sys.exit(1)

    # Ø§Ú¯Ø± Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ØŒ Ø¯Ø± Ø­Ø§Ù„Øª ØªÚ© Ø³ÙˆØ§Ù„ Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯
    if len(sys.argv) > 1:
        question = " ".join(sys.argv[1:])
        single_query_mode(question)
    else:
        # Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª Ø­Ø§Ù„Øª ØªØ¹Ø§Ù…Ù„ÛŒ
        interactive_mode()
